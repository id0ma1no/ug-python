{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "pd.set_option(\"display.float_format\", \"{:4.2g}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件的输入输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入输出函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|函数名      | 说明            |\n",
    "|:-------------|: ---------------------|\n",
    "|read_csv()   | 从csv文件读取数据        |\n",
    "|read_excel()  | 从Excel文件读取数据       |\n",
    "|HDFStore()   | 使用HDF5文件读写数据       |\n",
    "|read_sql()   | 从SQ数据库的查询结果载入数据 |\n",
    "|read_pickle()|  读入Pickle序列化后的数据   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_csv()的参数有: sep, header, skiprows, na_values, parse_dates, \n",
    "    encoding, usecols, chunksize()等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "#df_list是一个列表, 列表元素是DataFram对象, 每个DataFrame对象只有100行数据\n",
    "for df in pd.read_csv(\n",
    "        u\"data/aqi/上海市_201406.csv\", \n",
    "        encoding=\"utf-8-sig\",  #文件编码\n",
    "        chunksize=100,         #一次读入的行数\n",
    "        usecols=[u\"时间\", u\"监测点\", \"AQI\", \"PM2.5\", \"PM10\"], #只读入这些列\n",
    "        na_values=[\"-\", \"—\"],  #这些字符串表示缺失数据\n",
    "        parse_dates=[0]):      #第一列为时间列\n",
    "    df_list.append(df)  #在这里处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_list) #7000/100=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "时间       100\n",
       "监测点       90\n",
       "AQI      100\n",
       "PM2.5    100\n",
       "PM10      98\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "时间       datetime64[ns]\n",
       "监测点              object\n",
       "AQI               int64\n",
       "PM2.5             int64\n",
       "PM10            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_list[0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print type(df.loc[0, u\"监测点\"]) # 0行, 监测点列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建HDFStore对象, complib指定压缩数据库, complevel指定压缩级别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.io.pytables.HDFStore'>\n",
       "File path: a.hdf5\n",
       "/dataframes/df1                    frame        (shape->[100000,4])                                                 \n",
       "/dataframes/df2                    frame        (shape->[10000,3])                                                  \n",
       "/dataframes/df_dynamic1            frame_table  (typ->appendable,nrows->100000,ncols->4,indexers->[index],dc->[A,B])\n",
       "/series/s1                         series       (shape->[1000])                                                     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = pd.HDFStore(\"a.hdf5\", complib=\"blosc\", complevel=9)\n",
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFStore对象支持字典接口, 可用用[]存取元素, get(), key()等方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dataframes/df1', '/dataframes/df2', '/dataframes/df_dynamic1', '/series/s1']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(np.random.rand(100000, 4), columns=list(\"ABCD\"))\n",
    "df2 = pd.DataFrame(np.random.randint(0, 10000, (10000, 3)), \n",
    "                   columns=[\"One\", \"Two\", \"Three\"])\n",
    "s1 = pd.Series(np.random.rand(1000))\n",
    "\n",
    "#store是上面创建的一个HDFStore对象\n",
    "store[\"dataframes/df1\"] = df1 \n",
    "store[\"dataframes/df2\"] = df2\n",
    "store[\"series/s1\"] = s1\n",
    "\n",
    "print store.keys()\n",
    "print df1.equals(store[\"dataframes/df1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > http://pytables.github.io/usersguide/libref/hierarchy_classes.html\n",
    "\n",
    "> `pytables`项目介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_note()获得HDFStore对象的根节点, 然后调用_f_walknodes()遍历其包含的所有节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/ (RootGroup) ''\n",
       "  children := ['series' (Group), 'dataframes' (Group)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = store.get_node(\"//\")\n",
    "root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dataframes (Group) u''\n",
      "/series (Group) u''\n",
      "/dataframes/df1 (Group) u''\n",
      "/dataframes/df2 (Group) u''\n",
      "/series/s1 (Group) u''\n",
      "/series/s1/index (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/series/s1/values (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis0 (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis1 (CArray(100000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_items (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_values (CArray(100000, 4), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis0 (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis1 (CArray(10000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_items (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_values (CArray(10000, 3), shuffle, blosc(9)) ''\n"
     ]
    }
   ],
   "source": [
    "root = store.get_node(\"//\")\n",
    "for node in root._f_walknodes():\n",
    "    print node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append() 在原始目录中添加新的表格Table对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100100, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.append('dataframes/df_dynamic1', df1, append=False) \n",
    "# append = False 表示覆盖原先的df_dynamicl\n",
    "\n",
    "df3 = pd.DataFrame(np.random.rand(100, 4), columns=list(\"ABCD\"))\n",
    "\n",
    "store.append('dataframes/df_dynamic1', df3) #❷\n",
    "store['dataframes/df_dynamic1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dataframes (Group) ''\n",
      "/series (Group) ''\n",
      "/dataframes/df1 (Group) u''\n",
      "/dataframes/df2 (Group) u''\n",
      "/dataframes/df_dynamic1 (Group) u''\n",
      "/series/s1 (Group) u''\n",
      "/series/s1/index (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/series/s1/values (CArray(1000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis0 (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/axis1 (CArray(100000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_items (CArray(4,), shuffle, blosc(9)) ''\n",
      "/dataframes/df1/block0_values (CArray(100000, 4), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis0 (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/axis1 (CArray(10000,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_items (CArray(3,), shuffle, blosc(9)) ''\n",
      "/dataframes/df2/block0_values (CArray(10000, 3), shuffle, blosc(9)) ''\n",
      "/dataframes/df_dynamic1/table (Table(100100,), shuffle, blosc(9)) ''\n"
     ]
    }
   ],
   "source": [
    "root = store.get_node(\"//\")\n",
    "for node in root._f_walknodes():\n",
    "    print node\n",
    "    \n",
    "# 多了两行    /dataframes/df_dynamic1 (Group) u''\n",
    "# /dataframes/df_dynamic1/table (Table(100100,), shuffle, blosc(9)) ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dataframes/df1', '/dataframes/df2', '/dataframes/df_dynamic1', '/series/s1']\n"
     ]
    }
   ],
   "source": [
    "# 目前store中存储的键有哪些\n",
    "print store.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select()对表格进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A    B     C      D\n",
      "98  0.45 0.72   0.3   0.38\n",
      "99  0.73 0.34 0.064 0.0063\n",
      "100 0.35 0.13  0.17  0.034\n",
      "101 0.32 0.75   0.6   0.76\n",
      "98  0.24 0.25  0.54  0.031\n",
      "99  0.59 0.87  0.18   0.42\n"
     ]
    }
   ],
   "source": [
    "print store.select('dataframes/df_dynamic1', where='index > 97 & index < 102')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append()的另一个参数 data_columns 表示在创建新的表格时 指定索引列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A      B     C    D\n",
      "21223    1 0.0037  0.22 0.95\n",
      "26626 0.99  0.001  0.37 0.84\n",
      "32961 0.99 0.0094  0.77 0.48\n",
      "45890 0.99 0.0059  0.23 0.83\n",
      "66037 0.99 0.0033  0.52    1\n",
      "66611 0.99 0.0078 0.046  0.2\n",
      "93322    1 0.0079   0.6 0.77\n"
     ]
    }
   ],
   "source": [
    "store.append('dataframes/df_dynamic1', df1, append=False, data_columns=True)\n",
    "print store.select('dataframes/df_dynamic1', where='A > 0.99 & B < 0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**\n",
    "\n",
    "> 由于所有从CSV文件读入`DataFrame`对象的行索引都为缺省值，因此HDF5文件中的数据的行索引并不是唯一的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环读入data\\aqi下的scv到HDF5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HDF5文件不支持Unicode字符串\n",
    "* HDF5文件中的每列数据只能对应一种类型\n",
    "* 需要指定HDF5文件中字符串的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_aqi_files(fn_pattern):\n",
    "    from glob import glob\n",
    "    from os import path\n",
    "    \n",
    "#    变量名\n",
    "\n",
    "    UTF8_BOM = b\"\\xEF\\xBB\\xBF\"\n",
    "    # utf8编码文件的头三个字节\n",
    "    \n",
    "    cols = \"时间,城市,监测点,质量等级,AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")\n",
    "    # 列名列表\n",
    "    \n",
    "    float_dtypes = {col:float for col in \"AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")}\n",
    "    # 列名:类型,字典, 做dtype参数的\n",
    "    \n",
    "    names_map = {\"时间\":\"Time\", \n",
    "                 \"监测点\":\"Position\", \n",
    "                 \"质量等级\":\"Level\", \n",
    "                 \"城市\":\"City\", \n",
    "                 \"PM2.5\":\"PM2_5\"}\n",
    "    # 将所有的中文列名替换为为英文\n",
    "    \n",
    "#    循环操作csv文件\n",
    "\n",
    "    for fn in glob(fn_pattern):   #文件名样式, 所有的csv文件\n",
    "        with open(fn, \"rb\") as f:\n",
    "            \n",
    "            sig = f.read(3)     #1 这三行表示在BOM之后重设文件起点\n",
    "            if sig != UTF8_BOM: #2\n",
    "                f.seek(0, 0)    #3\n",
    "            \n",
    "            # 按指定格式读入csv文件\n",
    "            df = pd.read_csv(f, \n",
    "                             parse_dates=[0],       # 第0列表示日期列\n",
    "                             na_values=[\"-\", \"—\"], # 文件中表示NaN的符号 \n",
    "                             usecols=cols,          # 需要提取的列\n",
    "                             dtype=float_dtypes)  # 指定每一列的类型\n",
    "        df.rename_axis(names_map, axis=1, inplace=True) # 重设列名  \n",
    "        df.dropna(inplace=True)  # 删除所有包含NaN的行\n",
    "        yield df # 数据表对象df的生成器\n",
    "\n",
    "\n",
    "store = pd.HDFStore(\"data/aqi/aqi.hdf5\", \n",
    "                    complib=\"blosc\", complevel=9) # 创建HDFStore对象\n",
    "\n",
    "string_size = {\"City\": 12, \"Position\": 30, \"Level\":12} #设置store中字符串的长度\n",
    "\n",
    "for idx, df in enumerate(read_aqi_files(u\"data/aqi/*.csv\")):\n",
    "    store.append('aqi', df, append=idx!=0, min_itemsize=string_size, data_columns=True) #❸\n",
    "    # 不断往store中添加数据表\n",
    "store.close() # 关闭hdf5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  打开刚刚创建的HDFStore对象\n",
    "store = pd.HDFStore(\"data/aqi/aqi.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aqi']\n"
     ]
    }
   ],
   "source": [
    "# 查看所有的键\n",
    "print store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aqi (Group) ''\n",
      "/aqi/table (Table(337250,), shuffle, blosc(9)) ''\n"
     ]
    }
   ],
   "source": [
    "# 查看所有的节点\n",
    "root = store.get_node(\"//\")\n",
    "for node in root._f_walkNodes():\n",
    "    print node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337250\n"
     ]
    }
   ],
   "source": [
    "df_aqi = store.select(\"aqi\")\n",
    "print len(df_aqi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取PM2.5值大于500 的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "df_polluted = store.select(\"aqi\", where=\"PM2_5 > 500\")\n",
    "print len(df_polluted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读写数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///data/aqi/aqi.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    engine.execute(\"DROP TABLE aqi\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str_cols = [\"Position\", \"City\", \"Level\"]\n",
    "\n",
    "for df in read_aqi_files(\"data/aqi/*.csv\"):\n",
    "    for col in str_cols:\n",
    "        df[col] = df[col].str.decode(\"utf8\")\n",
    "    df.to_sql(\"aqi\", engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aqi = pd.read_sql(\"aqi\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "df_polluted = pd.read_sql(\"select * from aqi where PM2_5 > 500\", engine)\n",
    "print len(df_polluted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Pickle序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aqi.to_pickle(\"data/aqi/aqi.pickle\")\n",
    "df_aqi2 = pd.read_pickle(\"data/aqi/aqi.pickle\")\n",
    "df_aqi.equals(df_aqi2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
